In this seminar, I deployed a local large language model using Ollama and tested it under controlled security evaluation scenarios. I examined key generative AI threat categories, including prompt injection, simulated data poisoning, model inversion, and model extraction, by interacting with the model through Python scripts. Through these tests, I observed how the model responded to adversarial and repeated prompts and identified potential risks related to manipulation, privacy, and misuse. I also considered high-level mitigation strategies such as input sanitisation, access controls, monitoring, and governance to reduce security risks when deploying generative AI systems.
